---
layout: post
title: 2018-1 NLP 중간고사 정리
---

자연어 처리 중간고사가 2주전이다. 여기에다가 공부내용을 정리해둔다. ~~하지만 벌써 일주일전~~
(아래 내용은 https://web.stanford.edu/class/cs124/ , http://web.stanford.edu/~jurafsky/slp3/2.pdf, wikipedia,https://courses.engr.illinois.edu/cs447/fa2017/Slides/Lecture04.pdf를 참고했음)

# Lecture 1
## introduction of NLP
항상 그렇듯이 새로운 과목을 듣게 되면 처음에는 이 과목은 얼마나 유용하고 중요하고 기타 등등의 알면 ~~아는척 하기~~ 좋은 내용들이고 몰라도 ~~시험에는 안나오겠지~~ 상관 없으니 PASS! 할까 했지만 그래도 허전하니 link하나 추가
https://en.wikipedia.org/wiki/Natural-language_processing
추가로 하나만 적으면 NLP가 힘든 이유는  ambiguity 때문이라고 한다.

# Lecture 2
## Tokenization
Type: text에서 단어의 수
Token: text에서 얼마나 많이 반복되는 지 나타냄
따라서 token의 수는 type의 수보다 항상 크거나 같다

ex) Challenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation
Token : 13
type: 11 (natural-language이 반복됨)

Token과 type을 비율을 나타내는 값으로  TTR이 있다 (type/token ratio)
TTR = type/token

### Maximum matching.
내용이 어렵지 않으니 아래 slide로 갈음
![](https://lovetube.github.io/assets/images/maximum_matching.jpg)

### Lemmatization
: have to find correct dictionary headword form
#### Stemming
Reduce terms to their stems in information retrieval
e.g., automate(s), automatic, automation all reduced to automat.
#### Porter's algorithm
stemming하는 방법 중 대표적 방법
원문은 https://tartarus.org/martin/PorterStemmer/def.txt 에서 확인가능
![](https://lovetube.github.io/assets/images/porter_algorithm.jpg)

## Minimum Edit Distance
### Minimum Edit Distance 계산
자세한 algorithm은 아래와 같다
![](https://lovetube.github.io/assets/images/MinimumEditDistanceAlgorithm.jpg)

실제 손으로 해볼려면 아래와 같이 table을 만들어 해볼 수 있다  왼쪽과 아래쪽에 쓴다 #은 word가 없는 경우를 나타낸다 각각의 칸은 insert, delete substitution의 경우 cost를 계산하여 가장 작은 값을 적는다 각각의 칸을 채운 후 오른쪽 위 값을 Minimum Edit Distance로 return 한다
![](https://lovetube.github.io/assets/images/TheEditDistanceTable.jpg)
실제 결과는 아래와 같다 Intention과 Execution의 Minimum Edit Distance은 8임을 알 수 있다
![](https://lovetube.github.io/assets/images/TheEditDistanceTableResult.jpg)
### Backtrace
나름대로 정리한 방법은 대충 이렇다
1. 일단 Minimum Edit Distance를 만든다
2. 표의 각 cell에서 아래,왼쪽,대각선을 확인해서 같거나 작으면 화살표로 표시한다
3. 2.번 작업을 모든 cell에 대해 수행한다
4. 오른쪽 끝에서 왼쪽 아래로 화살표를 따라간다 여기서 중요한 점인데 **아래로 가는 길은 하나가 아니다.**
![](https://lovetube.github.io/assets/images/MinEditwithBacktrace.jpg)

# Lecture 3
## N-Gram
* N-gram is a contiguous sequence of n items from a given sample of text or speech
* An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.
--> which predict the next word from the previous N-1 words
* 2-grams (aka bigrams)
--> (I notice), (notice three), (three guys), (guys standing), (standing on), (on the)
## language model
* A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability $$ P(w_1,... ,w_m)$$ to the whole sequence.

$$P(w_5|w_1,w_2,w_3,w_4)$$ →  $$w_1,w_2,w_3,w_4$$가 연속되었을 때 $$w_5$$가 나올 확률을 나타낸다

## The Chain rule
위 language model을 예측하기 위해서는 아래 식을 계산하면 된다 ~~이론적으로는 말이지~~
![](https://lovetube.github.io/assets/images/chain_rule.jpg)
## Makov Assumption
이론적으로 언어모델을 계산할 수 있으나 문장이 조금만 길어저도 ~~어디한번 해보시지~~ 계산이 기하급수적으로 늘어난다. 이렇게 되면 실제로 사용하기 힘들어진다. 따라서 약간의(?) 가정을 해서 언어모델을 찾는데 이때 사용한 가정을 Makov Assumption이라 한다 **모든 확률을 다 계산하지 말고 N개의 단어가 연속할때 특정단어가 나올 확률을 구하는 것**
![](https://lovetube.github.io/assets/images/makov_assumption.jpg)
## Maximum Likelihood Estimates
어떤 모수가 주어졌을 때, 원하는 값들이 나올 가능도를 최대로 만드는 모수를 선택하는 방법이다.
bigram일때는 아래 공식으로 계산할 수 있다
![](https://lovetube.github.io/assets/images/maximum_likelihood_estimates.jpeg)

## The Shannon Visualization Method
![](https://lovetube.github.io/assets/images/ShannonVisualizationMethod.jpeg)

## Evaluation
ML에서 그렇듯 해당 model을 평가하기 위해서는 training set / test set을 나누고 training set을 위해 학습시키고 test set을 가지고 평가한다. 하지만 어떻게?

### Extrinsic Evaluation (in vivo evaluation)
* Extrinsic evaluation of word vectors is the evaluation of a set of word vectors generated by an embedding technique on the real task at hand. **These tasks are typically elaborate and slow to compute.** Typically, optimizing over an underperforming extrinsic evaluation system does not allow us to determine which specific subsystem is at fault and this motivates the need for intrinsic evaluation.

### Perplexity
![](https://lovetube.github.io/assets/images/Perplexity.jpeg)

# Lecture 4
* 우리의 corpus 제한적이니 valid word sequence도 우리의 corpus에 없을 수 있고 결국 zero probability가 나올수 있다. 이를 방지하기 위해 아래의 방법을 사용할 수 있다 ~~그냥 sample을 늘리자~~

## Laplace (Add-1)
* add-1은 unseen event에 대해 나무 많은 가능성을 부여한다
![](https://lovetube.github.io/assets/images/laplace_smoothing.jpeg)
![](https://lovetube.github.io/assets/images/reconstituting_counts.jpg)
![](https://lovetube.github.io/assets/images/Add-K-smoothing.jpg)
![](https://lovetube.github.io/assets/images/Summary-Add-One-smoothing.jpg)

## Good-Turning
![](https://lovetube.github.io/assets/images/good_turing_1.jpg)
![](https://lovetube.github.io/assets/images/good_turing_2.jpg)
![](https://lovetube.github.io/assets/images/good_turing_3.jpg)

## Interpolation
TBD: 기본 개념은 모르는거 나오면 unigram, bigram, trigram 다 써서 맞추어 보자

## Backoff
TBD: 일단 N을 모르면 N-1까지 있는 걸로 알아 맞추어 보자

# Lecture 5
일단 아래는 이번 주말까지 업데이트 하는 걸로
https://en.wikipedia.org/wiki/Part_of_speech

## Part of Speech
## Targets
## Part of speech tagging
## HMM Tagging
## Evaluation
